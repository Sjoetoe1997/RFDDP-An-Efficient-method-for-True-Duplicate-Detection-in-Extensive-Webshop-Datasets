# RFDDP-An-Efficient-method-for-True-Duplicate-Detection-in-Extensive-Webshop-Datasets
As part of the Computer Science course at Erasmus University in Rotterdam, students were tasked with tackling the scalability challenges associated with detecting duplicate products using data from multiple web shops. The dataset, accessible at https://personal.eur.nl/frasincar/datasets/TVs-allmerged.zip, contain 1,624 television descriptions from four distinct web shops: Amazon.com, Newegg.com, Best-Buy.com, and TheNerds.net. Each television entry includes shop information, title, modelID, and a set of key-value paired features.

The algorithm for duplicate detection undergoes a thorough process. Initial data preprocessing involves standardizing variations of 'inch' and 'herz' in title and value parts. Subsequently, a product representation for Locality-Sensitive Hashing+ (LSH+) is created, using model words extracted from the title with a specific nature. These model words serve as the foundation for generating binary vectors that represent the individual products. Subsequently, these binary vectors undergo the creation of a signature matrix through minhashing, a technique used to condensing sizable and often sparse binary vectors into signature vectors utilizing a hash function. In our context, the hash functions take the form of (a + bx) mod (p), where 'a' and 'b' are random integers, and 'p' is a randomly selected prime number. The ultimate purpose of these signatures is to facilitate a precise and rapid estimation of the Jaccard similarity between two sets. This is done for threshold values ranging from 0.25 to 0.95 withn steps of 0.05, where the threshold is related to the formula: (1. / b) ** (1. / r).

Subsequently, 10 bootstrap samples are generated by randomly spliting the data into 63% train and 37% test data. Next, LSH+ is used to create candidate pairs for both the 10 test and the 10 train sets, wherafter the candidates used to obtain true duplicate through random forrest binary classification. After this is done for all 10 bootstraps, the F1 and F1* values of each bootstrap are averaged to reduce bias.

Finally all relevant measures are plotted

. 
